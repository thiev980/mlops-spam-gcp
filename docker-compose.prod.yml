name: mlops-spam-prod

networks:
  airflow_net:
    name: airflow_net

services:
  postgres:
    image: postgres:16
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow_prod
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow_prod"]
      interval: 5s
      timeout: 5s
      retries: 10
    ports:
      - "5432:5432"
    networks: [airflow_net]
    volumes:
      - pgdata_prod:/var/lib/postgresql/data
      - ./postgres-init:/docker-entrypoint-initdb.d

  redis:
    image: redis:7
    container_name: airflow-redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 10
    ports:
      - "6379:6379"
    networks: [airflow_net]

  airflow-init:
    image: my-airflow:2.9.2
    depends_on:
      postgres: { condition: service_healthy }
      redis: { condition: service_healthy }
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
    # entrypoint: ["bash","-lc"]   <-- WEG DAMIT
    command: ["bash","-lc","airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true"]
    networks: [airflow_net]
    volumes:
      - ./airflow-docker/dags:/opt/airflow/dags
      - ./airflow-docker/models-prod:/opt/airflow/models
      - ./airflow-docker/data-prod:/opt/airflow/data
      - ./airflow-docker/logs-prod:/opt/airflow/logs

  airflow-web:
    image: my-airflow:2.9.2
    container_name: airflow-web
    depends_on:
      postgres: { condition: service_healthy }
      redis: { condition: service_healthy }
      airflow-init: { condition: service_completed_successfully }
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: Europe/Zurich
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    ports: ["8080:8080"]
    command: webserver
    networks: [airflow_net]
    volumes:
      - ./airflow-docker/dags:/opt/airflow/dags
      - ./airflow-docker/models-prod:/opt/airflow/models
      - ./airflow-docker/data-prod:/opt/airflow/data
      - ./airflow-docker/logs-prod:/opt/airflow/logs

  airflow-scheduler:
    image: my-airflow:2.9.2
    container_name: airflow-scheduler
    depends_on:
      postgres: { condition: service_healthy }
      redis: { condition: service_healthy }
      airflow-init: { condition: service_completed_successfully }
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    command: scheduler
    networks: [airflow_net]
    volumes:
      - ./airflow-docker/dags:/opt/airflow/dags
      - ./airflow-docker/models-prod:/opt/airflow/models
      - ./airflow-docker/data-prod:/opt/airflow/data
      - ./airflow-docker/logs-prod:/opt/airflow/logs

  airflow-worker:
    image: my-airflow:2.9.2
    container_name: airflow-worker
    depends_on:
      postgres: { condition: service_healthy }
      redis: { condition: service_healthy }
      airflow-init: { condition: service_completed_successfully }
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CELERY__BROKER_URL: ${AIRFLOW__CELERY__BROKER_URL}
      AIRFLOW__CELERY__RESULT_BACKEND: ${AIRFLOW__CELERY__RESULT_BACKEND}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    command: celery worker
    networks: [airflow_net]
    volumes:
      - ./airflow-docker/dags:/opt/airflow/dags
      - ./airflow-docker/models-prod:/opt/airflow/models
      - ./airflow-docker/data-prod:/opt/airflow/data
      - ./airflow-docker/logs-prod:/opt/airflow/logs

  grafana:
    image: grafana/grafana:10.4.2
    container_name: airflow-grafana
    depends_on:
      postgres: { condition: service_healthy }
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    ports: ["3000:3000"]
    networks: [airflow_net]
    volumes:
      - grafana_data_prod:/var/lib/grafana

  mlflow:
    build:
      context: .
      dockerfile: airflow-docker/Dockerfile.mlflow
    container_name: mlflow
    depends_on:
      postgres: { condition: service_healthy }
    environment:
      MLFLOW_ARTIFACTS_DESTINATION: /mlartifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5002
      --backend-store-uri postgresql+psycopg2://airflow:airflow@postgres/mlflow_prod
      --serve-artifacts
      --artifacts-destination /mlartifacts
    ports: ["5002:5002"]
    networks: [airflow_net]
    volumes:
      - mlartifacts_prod:/mlartifacts

volumes:
  pgdata_prod:
  grafana_data_prod:
  mlartifacts_prod:
  # optional – nur falls du später auch Logs/Data auf Named Volumes legen willst:
  # airflow_logs_prod:
  # airflow_data_prod: